import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer, StringIndexer, IndexToString}
import org.apache.spark.ml.classification.NaiveBayes
import org.apache.spark.ml.feature.StopWordsRemover
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.RegexTokenizer

//import dataset in cache

var df=spark.read.format("csv").option("header","true").option("delimiter","|").load("/home/master/Desktop/inputprovapipe2.csv")
df.registerTempTable("mal")
var df_train=spark.sql("select * from mal where tipo_lista='TRAINING'")

var df_test=spark.sql("select * from mal where tipo_lista='TEST'")

df_test.cache()
df_train.cache()

df.show()



// indexer
val indexer = new StringIndexer().setInputCol("labelpers").setOutputCol("label").fit(df_train)

//tokenizer

val tokenizer = new Tokenizer().setInputCol("commento").setOutputCol("words")


//test

val tokenized = tokenizer.transform(df_train)

val tokens = tokenized.select(explode(col("words")).as("word")).groupBy("word").count().orderBy(desc("count"))

tokens.show(false)


// uso stopwordremover in italiano



val stopWordsRemover = new StopWordsRemover().setInputCol("words").setOutputCol("clean_words").setStopWords(StopWordsRemover.loadDefaultStopWords("italian"))
val cleaned = stopWordsRemover.transform(tokenized)

// check parole uniche  rimanenti

val tokens = cleaned.select(explode(col("clean_words")).as("word")).groupBy("word").count().orderBy(desc("count"))
tokens.show(false)

// check numero parole totali rimanenti

val tokens = cleaned.select(explode(col("clean_words")).as("word")).count()
tokens.show(false)

// miglioro a tokenizzazione --> mettere il pattern regex corretto

val tokenizer = new RegexTokenizer().setInputCol("commento").setOutputCol("words").setPattern("[a-zA-Z]")
val tokenized = tokenizer.transform(df_train)
val cleaned = stopWordsRemover.transform(tokenized)
val tokens = cleaned.select(explode(col("clean_words")).as("word")).groupBy("word").count().orderBy(desc("count"))
tokens.show(false)
